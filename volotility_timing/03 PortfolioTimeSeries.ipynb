{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PortfolioTimeSeries.ipynb\n",
    "\n",
    "Code for the Chicago Booth course on Quantitative Portfolio Management by Ralph S.J. Koijen and Federico Mainardi."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "This code builds time-series portfolio strategies. *This notebook also contains the questions for problem set 2.*\n",
    "- As always, the data can be found in the dropbox folder: You can download the data from: https://www.dropbox.com/scl/fo/hrjspow2cpstfnoeqb23v/h?rlkey=j4fohf1s4e6fdy49p7bs71b7l&dl=0.\n",
    "- Please download the file `ETFdata_small.parquet`. \n",
    "\n",
    "We first load several packages to initialize Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wrds \n",
    "import qpm\n",
    "import qpm_download\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select wether you would like to download data directly from WRDS (`import_data` = True) or to load data from Dropbpx (`import_data` = False). If you decide to load data from Dropbox, make sure to define the data directoy (`_DATA_DIR`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_data = False             # <-- Edit this line\n",
    "_DATA_DIR = '../Data'           # <-- Edit this line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select the sample period and ticker here. We can select an ETF covering the financial sector (ticker: XLF) or the S&P500 (ticker: SPY). To keep the data files initially small so that the code runs quickly, the file only contains two tickers. \n",
    "\n",
    "However, we can adjust this to any ticker. The file `ETFdata.parquet` contains thousands of ETFs and you can use these data for your final project. These ETFs allow you to explore other assets classes, industries or countries. For an overview of available ETFs, you can for instance visit this website: https://etfdb.com/etfs/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the sample period\n",
    "_SAMPLE_START = '2005-01-01'\n",
    "_SAMPLE_END = '2023-07-31'\n",
    "\n",
    "# Select the ticker\n",
    "_ETF_TICKER = 'SPY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to start!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Load Data\n",
    "\n",
    "We first load the *daily* ETF data and we store it in `df_ETF`. If you selected import_data = True, the code will download and construct the data directly from WRDS. Otherwise, the code will import data that you downloaded from the Dropbox folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if import_data == True: \n",
    "    \n",
    "    df_ETF_raw = qpm_download.time_series(_SAMPLE_START, _SAMPLE_END)\n",
    "        \n",
    "if import_data == False:\n",
    "    \n",
    "    # Load the data\n",
    "    df_ETF_raw = pd.read_parquet('%s/ETFdata_small.parquet' %(_DATA_DIR))    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ETF, we have the daily return, `retd`, and the return of that month, `retM`. We use the daily return to compute the standard deviation below. The variable `date` is the daily date; the variable `ym` is the year and month. \n",
    "\n",
    "The factors (`mktrf` = excess return on the market, `smb` = the Fama and French size factor, `hml` = the Fama and French value factor, `rf` = the risk-free rate, `umd` = the momentum factor) are all *monthly* variables. In the dataset available in Dropbox, we also include the `CPI`, `inflation`, and `yr5breakeven`, which is the 5-year break-even inflation rate. We include those in case you like to explore strategies related to inflation risk, which is an important theme in the industry at the moment. We drop these data for this problem set as we don't need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the ETF using the ticker symbol\n",
    "df_ETF = df_ETF_raw[df_ETF_raw['ticker'] == _ETF_TICKER]\n",
    "\n",
    "# Select the relevant variables for our strategy\n",
    "df_ETF = df_ETF[['date', 'ym', 'retd', 'retM', 'mktrf', 'rf']]\n",
    "\n",
    "# Sort the data\n",
    "df_ETF.sort_values(['date'], inplace = True)\n",
    "\n",
    "# Print the data \n",
    "df_ETF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Portfolio Construction\n",
    "\n",
    "To compute the standard deviation, we first group the data by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_ETF.groupby('ym')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the statistic we need per month for each variable. For the daily returns, we need the standard deviation. For the monthly returns, we just pick the first observation (as those variables are constant within a given month). The following code then computes these statistics and provides a dataset that has one observation per month. We call the standard deviation of daily returns `sd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grouped.agg({\n",
    "    'retd': 'std',\n",
    "    'retM': 'first',\n",
    "    'rf': 'first',\n",
    "    'mktrf': 'first'\n",
    "})\n",
    "df = df.rename(columns={'retd': 'sd'})\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can only form portfolios based on information that we know in advance. For instance, for the portfolio that we hold in October 2023, we can only use data that we know in September 2023. We therefore lag the volatility signal by one period. \n",
    "- We compute the lag using `shift` as we have seen in the mean-variance problem set. We then annualize the standard deviation by the square root of the number of trading days, $\\sqrt{252}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lsd'] = df['sd'].shift(1) * np.sqrt(252)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one missing value created, but we will take care of this in constructing the portfolio weights.\n",
    "\n",
    "We are now ready to compute the portfolio weight as\n",
    "$$w(t) = \\min\\left\\{\\frac{c}{\\sigma_t},2\\right\\},$$\n",
    "where $c$ is a scaling factor. It determines the amount of risk we'd like to take. Higher values of $c$ correspond to riskier strategies. We want to have an average $\\beta$ of about 1. We choose $c=1.25 \\times \\overline{\\sigma_t}$, where $\\overline{\\sigma_t}$ is the average volatility, $\\sigma_t$. We take 1.25 so that the average $\\beta$ is around one. We take the minimum of $\\frac{c}{\\sigma_t}$ and 2 to avoid that the portfolio gets too extreme during low volatility periods. \n",
    "\n",
    "The code has three steps\n",
    "1. Compute the scaling $c$.\n",
    "2. Compute the portfolio weight. Note that in computing the portfolio weight, it ends with `fillna(1)`. This means that missing portfolio weights (caused by the missing value of the standard deviation above) are replaced with 1, that is, invest all capital in the ETF.\n",
    "3. In the final step, we drop the data from the standard deviation (`sd`), the lagged standard deviation (`Lsd`) and the scaling factor (`c`), as we no longer need those for the remainder of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c'] = 1.25 * df['Lsd'].mean()\n",
    "df['weight'] = np.minimum(df['c'] / df['Lsd'], 2).fillna(1)\n",
    "df.drop(columns = ['sd','Lsd', 'c'], inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the return on the portfolio\n",
    "$$r_p(t) = w(t) r_{ETF}(t)+[1-w(t)]r_f(t).$$\n",
    "The first line computes the raw return, and the next two lines compute the excess return by subtracting the risk-free rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns on the volatility timing strategy\n",
    "df['retP'] = df['weight'] * df['retM'] + (1 - df['weight']) * df['rf']\n",
    "\n",
    "# Excess returns on the volatility timing strategy\n",
    "df['reteP'] = df['retP'] - df['rf']\n",
    "\n",
    "# Excess returns on the ETF itself (without volatility timing)\n",
    "df['reteM'] = df['retM'] - df['rf']\n",
    "\n",
    "# Drop the risk-free rate from the data as we no longer need it\n",
    "df.drop(columns = ['rf'], inplace = True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done building the strategy! Let's analyze the returns now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Portfolio Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first select the sample period that we would like to study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.index >= _SAMPLE_START) & (df.index <= _SAMPLE_END)]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the portfolio weights to see what the strategy does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df.index, df['weight'], linewidth=2.0)\n",
    "plt.ylabel('Date')\n",
    "plt.ylabel('Portfolio weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cumulative returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qpm.plot_cumulative_returns_etf(df, var_list = ['retM', 'retP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block computes the *annualized* average return, the standard deviation, and Sharpe ratio for each of the strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics, and only keep the mean and the standard deviation\n",
    "summary = df[['reteM', 'reteP']].describe().T[['mean', 'std']]\n",
    "\n",
    "# Annualize the mean\n",
    "summary['mean'] = summary['mean'] * 12\n",
    "\n",
    "# Annualize the standard deviation\n",
    "summary['std'] = summary['std'] * np.sqrt(12)\n",
    "\n",
    "# Compute the Sharpe ratio\n",
    "summary['sr'] = summary['mean'] / summary['std']\n",
    "\n",
    "# Print the mean, standard deviation, and Sharpe ratio\n",
    "print(summary.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see whether the strategy using `SPY` generates alpha. We use the market excess return as our benchmark. You can change the end of the sample by modifying the date `2019-12-01`. In the first case, we stop before 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the sample\n",
    "reg_df_select = df[df.index <= '2019-12-01']\n",
    "\n",
    "# Run a regression of reteP (the excess return on the vol timed strategy) on the excess return of the market\n",
    "result = sm.ols('reteP ~ mktrf', data=reg_df_select).fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret the output. There are 180 observations, that is, 180 months. The constant is the alpha, and it equals 0.39% per month. This implies an annual alpha of 12 * 0.39 = 4.68%. The corresponding t-statistic is 2.35. This implies that the alpha is signifficantly positive at conventional levels (-1.96 and 1.96 are the usual cutoffs).\n",
    "\n",
    "The beta is 1.03 (the coefficient corresponding to mktrf). It is also highly significant. Hence, we here have a strategy that simply times the market based on volatility and it generates an alpha of almost 5% a year. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Run the same regression but now including the data from 2020, the year of the COVID downturn (and recovery). Use the code in the previous cell and change the date to `2020-12-31`. How does the performance change and what is the economic interpretation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore the financial markets ETF. We change the ticker to `XLF` and run the same code as before. We collect the relevant code in the cell below. You can run it without modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the code from above but now for XLF\n",
    "\n",
    "_ETF_TICKER = 'XLF'\n",
    "\n",
    "# Load the data \n",
    "df_ETF = df_ETF_raw[df_ETF_raw['ticker'] == _ETF_TICKER]\n",
    "df_ETF.sort_values(['date', 'ym'], inplace = True)\n",
    "\n",
    "# Compute the standard deviation and keep one observation per month\n",
    "grouped = df_ETF.groupby('ym')\n",
    "df = grouped.agg({\n",
    "    'retd': 'std',\n",
    "    'retM': 'first',\n",
    "    'rf': 'first',\n",
    "    'mktrf': 'first'\n",
    "})\n",
    "df = df.rename(columns={'retd': 'sd'})\n",
    "\n",
    "# Compute the lagged standard deviation \n",
    "df['Lsd'] = df['sd'].shift(1) * np.sqrt(252)\n",
    "\n",
    "# Compute the scaling\n",
    "df['c'] = 1.25 * df['Lsd'].mean()\n",
    "\n",
    "# Compute the portfolio weight\n",
    "df['weight'] = np.minimum(df['c'] / df['Lsd'], 2).fillna(1)\n",
    "\n",
    "# Compute strategy returns, excess strategy returns, and the excess return on the ETF itself\n",
    "df['retP'] = df['weight'] * df['retM'] + (1 - df['weight']) * df['rf']\n",
    "df['reteP'] = df['retP'] - df['rf']\n",
    "df['reteM'] = df['retM'] - df['rf']\n",
    "\n",
    "# Select the sample\n",
    "df = df[(df.index >= _SAMPLE_START) & (df.index <= _SAMPLE_END)]\n",
    "\n",
    "reg_df_select = df\n",
    "\n",
    "print(\"Done rebuilding the data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that `reteM` is the excess return on the untimed ETF. We first measure the performance of this ETF, untimed, by regressing it on the benchmark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress the ETF excess return (untimed) on the excess return on the market\n",
    "result = sm.ols('reteM ~ mktrf', data=reg_df_select).fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underperformance is quite large. The alpha is -0.5% per month, or -6.0% per year. The alpha is statistically significant. Now let's explore whether volatility timing helps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "Consider a performance regression of the volatility-timed XLF using the same benchmark, as before. For the sample, we can use the full sample. Describe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Repeat this exercise, but now using the untimed XLF returns as the benchmark, and the full sample. Describe the economic and statistical significance of the results. For which investors would the results in this analysis be particularly relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Insert your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
